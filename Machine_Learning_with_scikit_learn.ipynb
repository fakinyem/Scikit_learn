{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n_5oRe0SXilM",
        "9SxiIczg1s1k",
        "5PbjhPxLmsI4",
        "ROFPolZpm21t",
        "qBXGs0xRERuv",
        "AD6zwuTHiYKA",
        "NVSRftm8X1m1",
        "pclZR6uFklf_",
        "8UQgU5I-lEll",
        "FJ5rjq7fIe8Q",
        "q7CNxkPdNB4L",
        "ITfbaOgfYNsq",
        "x2NWxK0BFwyw",
        "RHRXds9U9134",
        "HZ5LFPmmp1Gb",
        "K4qgOdz7Yyeb",
        "7-CGSS2OZKHD",
        "EHZ-hHGuY5aG",
        "vhhycm2S6wbz",
        "jFnEt7Lbr4_4",
        "v9TSuObo950Z",
        "PSyJzomm950e",
        "b3VoJKsVeff0",
        "qd9WGjVQ951B",
        "9O3ArMG6eff4",
        "YDsRQaj-r61V",
        "cXqNWQWT91kU",
        "hF-FTX_P91ka",
        "4rP48tTL91nz",
        "wPOM-hCV91pB",
        "bz2LEGDP91qM",
        "GmI6BdVg8RB7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fakinyem/Scikit_learn/blob/main/Machine_Learning_with_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_5oRe0SXilM"
      },
      "source": [
        "# Introduction to machine learning & Data Analysis\n",
        "\n",
        "Basic introduction on how to perform typical machine learning tasks with scikit-learn package in Python.\n",
        "\n",
        "Prepared by Mykhailo Vladymyrov, Data Science Lab, University Of Bern, 2023\n",
        "\n",
        "Partially based on earlier material by M. Vladymyrov & A. Marcolongo.\n",
        "\n",
        "This work is licensed under <a href=\"https://creativecommons.org/share-your-work/public-domain/cc0/\">CC0</a>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SxiIczg1s1k"
      },
      "source": [
        "# What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PbjhPxLmsI4"
      },
      "source": [
        "## Learning from data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsd1MyT9eIdW"
      },
      "source": [
        "Unlike classical algorithms, created by a human to analyze some data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtoqE5XO3L1j"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_1.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4Uq4k_ePoo"
      },
      "source": [
        "in machine learning we use the data directly to define the algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2xIgm223vfa"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvAyI1uzfBUT"
      },
      "source": [
        "Machine learning - is learnign from data.\n",
        "If\n",
        "* performance on a task **T**\n",
        "* improves according to measure **P**\n",
        "* with experience **E**\n",
        "\n",
        "we say that model learns form data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YsgnbD32dk"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/alg_3.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to8vOJeC1xjE"
      },
      "source": [
        "\n",
        "The boundary is a bit fuzzy.\n",
        "\n",
        "In fact when we create algorithms, the problem in hand (namely the data  related to the problem), drives us to choose one or another algorithm. And we then tune it, to perform well on a task in hand.\n",
        "\n",
        "Machine Learning (ML) formalized this procedure, allowing us to automate (part) of this process.\n",
        "\n",
        "In this course you will get acquainted with the basics of Machine Learning â€” where the approach to handling the data (the algorithm) is defined, or as we say \"learned\" from data in hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROFPolZpm21t"
      },
      "source": [
        "## Classification vs Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70_dMCX340Rm"
      },
      "source": [
        "The two main tasks handled by (supervised) ML is regression and classification.\n",
        "In regression we aim at modeling the relationship between the system's response (dependent variable) and one or more explanatory variables (independent variables).\n",
        "\n",
        "Examples of regression would be predicting the temperature for each day of the year, or expenses of the household as a function of the number of children and adults.\n",
        "\n",
        "In classification the aim is to identify what class does a data-point belong to. For example, the species or the iris plant based on the size of its petals, or whether an email is spam or not based on its content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBXGs0xRERuv"
      },
      "source": [
        "## Performance measures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx37P09Vkepw"
      },
      "source": [
        "1. Regression:\n",
        "* Mean Square Error: $\\textrm{MSE}=\\frac{1}{n}\\sum_i(y_i - \\hat y(\\bar x_i))^2$\n",
        "* Mean Absolute Error: $\\textrm{MAE}=\\frac{1}{n}\\sum_i|y_i - \\hat y(\\bar x_i)|$\n",
        "* Median Absolute Deviation: $\\textrm{MAD}=\\textrm{median}(|y_i - \\hat y(\\bar x_i)|)$\n",
        "* Fraction of the explained variance: $R^2=1-\\frac{\\sum_i(y_i - \\hat y(\\bar x_i))^2}{\\sum_i(y_i - \\bar y_i)^2}$, where $\\bar y=\\frac{1}{n}\\sum_i y_i$\n",
        "\n",
        "2. Classification:\n",
        "* Confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSH3blOw36jz"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/confusion_mtr.png\" width=\"46%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK2gGVJyfdUJ"
      },
      "source": [
        "* Accuracy $=\\frac{\\textrm{TP} + \\textrm{TN}}{\\textrm{TP} + \\textrm{FP} + \\textrm{FN} + \\textrm{TN}}$\n",
        "* Precision $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FP}}$\n",
        "* Recall $=\\frac{\\textrm{TP}}{\\textrm{TP} + \\textrm{FN}}$\n",
        "* F1 $=\\frac{2}{\\frac{1}{\\textrm{Precision}} + \\frac{1}{\\textrm{Recall}} } = 2\\frac{\\textrm{Precision} \\cdot \\textrm{Recall}}{\\textrm{Precision} + \\textrm{Recall}} = \\frac{2 \\textrm{TP}}{2 \\textrm{TP} + \\textrm{FP} + \\textrm{FN}} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\frac{\\textrm{FP} + \\textrm{FN}}{2}}$\n",
        "* F2 $=\\frac{5}{\\frac{1}{\\textrm{Precision}} + \\frac{4}{\\textrm{Recall}} } = 5\\frac{\\textrm{Precision} \\cdot \\textrm{Recall}}{4 \\cdot \\textrm{Precision} + \\textrm{Recall}} = \\frac{5 \\textrm{TP}}{5 \\textrm{TP} + \\textrm{FP} + 4 \\cdot \\textrm{FN}} = \\frac{\\textrm{TP}}{\\textrm{TP} + \\frac{\\textrm{FP} + 4 \\cdot\\textrm{FN}}{5}}$\n",
        "* Threat score (TS), or Intersection over Union: $\\mathrm{IoU}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}+\\mathrm{FP}}$\n",
        "\n",
        "\n",
        "During model optimization the used measure in most cases must be differentiable. To this end usually some measure of similarities of distributions are employed (e.g. cross-entropy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD6zwuTHiYKA"
      },
      "source": [
        "## Actual aim: Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsD3FQS4JP7"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_1.png\" width=\"35%\"/>\n",
        "\n",
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/Bias_variance_2.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoONru7ji3QD"
      },
      "source": [
        "To measure model performance in an unbiassed way, we need to use different data than the data that the model was trained on. For this we use the 'train-test' split: e.g. 20% of all available dataset is reserved for model performance test, and the remaining 80% is used for actual model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSRftm8X1m1"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVJn0ilgOS8F",
        "scrolled": true
      },
      "source": [
        "# Scikit-learn (formerly scikits.learn and also known as sklearn) is a free\n",
        "# software machine learning library for the Python programming language.\n",
        "# It features various classification, regression and clustering algorithms,\n",
        "# and is designed to interoperate with the Python numerical and scientific\n",
        "# libraries NumPy and SciPy. (from wiki)\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import tree\n",
        "from sklearn import ensemble\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# common visualization module\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# numeric library\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from time import time as timer\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from IPython.display import SVG\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn"
      ],
      "metadata": {
        "id": "Z2ABgGsF3RDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0LDjc5nECH"
      },
      "source": [
        "pip install dtreeviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gFMn8yunHDI"
      },
      "source": [
        "import dtreeviz\n",
        "import umap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y7aMevU3Ug8"
      },
      "source": [
        "if not os.path.exists('data'):\n",
        "    path = os.path.abspath('.')+'/colab_material.tgz'\n",
        "    tf.keras.utils.get_file(path, 'https://raw.githubusercontent.com/dsl-unibe-ch/ML_with_scikit-learn/main/colab_material.tgz')\n",
        "    tar = tarfile.open(path, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.routines import *"
      ],
      "metadata": {
        "id": "kHcYrI1S3aeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pclZR6uFklf_"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_wxOrdWko8W"
      },
      "source": [
        "In this course we will use several synthetic and real-world datasets to illustrate the behavior of the models and exercise our skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UQgU5I-lEll"
      },
      "source": [
        "## 1. Synthetic linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGfWOWRjlWPa"
      },
      "source": [
        "def get_linear(n_d=1, n_points=10, w=None, b=None, sigma=5):\n",
        "  x = np.random.uniform(0, 10, size=(n_points, n_d))\n",
        "\n",
        "  w = w or np.random.uniform(0.1, 10, n_d)\n",
        "  b = b or np.random.uniform(-10, 10)\n",
        "  y = np.dot(x, w) + b + np.random.normal(0, sigma, size=n_points)\n",
        "\n",
        "  print('true slopes: w =', w, ';  b =', b)\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RLYxGy_nBZG",
        "outputId": "5aef30a2-3a28-4258-d6a5-440cfc295c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=1)\n",
        "plt.plot(x[:, 0], y, '*')\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2254f130faf7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a414eb21fa92>\u001b[0m in \u001b[0;36mget_linear\u001b[0;34m(n_d, n_points, w, b, sigma)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ODDOp4nX4S"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=100)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x[:,0], x[:,1], y, marker='x', color='b',s=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ5rjq7fIe8Q"
      },
      "source": [
        "## 2. House prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-45usskInlD"
      },
      "source": [
        "Subset of the Ames Houses dataset: http://jse.amstat.org/v19n3/decock.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVv2ID96IyN0"
      },
      "source": [
        "def house_prices_dataset(return_df=False, return_df_xy=False, use_dummies=False, price_max=400000, area_max=40000):\n",
        "  path = 'data/AmesHousing.csv'\n",
        "\n",
        "  df = pd.read_csv(path, na_values=('NaN', ''), keep_default_na=False,  )\n",
        "\n",
        "  rename_dict = {k:k.replace(' ', '').replace('/', '') for k in df.keys()}\n",
        "  df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "  useful_fields = ['LotArea',\n",
        "                  'Utilities', 'OverallQual', 'OverallCond',\n",
        "                  'YearBuilt', 'YearRemodAdd', 'ExterQual', 'ExterCond',\n",
        "                  'HeatingQC', 'CentralAir', 'Electrical',\n",
        "                  '1stFlrSF', '2ndFlrSF','GrLivArea',\n",
        "                  'FullBath', 'HalfBath',\n",
        "                  'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
        "                  'Functional','PoolArea',\n",
        "                  'YrSold', 'MoSold'\n",
        "                  ]\n",
        "  target_field = 'SalePrice'\n",
        "\n",
        "  df.dropna(axis=0, subset=useful_fields+[target_field], inplace=True)\n",
        "\n",
        "  cleanup_nums = {'Street':      {'Grvl': 0, 'Pave': 1},\n",
        "                  'LotFrontage': {'NA':0},\n",
        "                  'Alley':       {'NA':0, 'Grvl': 1, 'Pave': 2},\n",
        "                  'LotShape':    {'IR3':0, 'IR2': 1, 'IR1': 2, 'Reg':3},\n",
        "                  'Utilities':   {'ELO':0, 'NoSeWa': 1, 'NoSewr': 2, 'AllPub': 3},\n",
        "                  'LandSlope':   {'Sev':0, 'Mod': 1, 'Gtl': 3},\n",
        "                  'ExterQual':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'ExterCond':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'BsmtQual':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtCond':    {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'BsmtExposure':{'NA':0, 'No':1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n",
        "                  'BsmtFinType1':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'BsmtFinType2':{'NA':0, 'Unf':1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ':5, 'GLQ':6},\n",
        "                  'HeatingQC':   {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'CentralAir':  {'N':0, 'Y': 1},\n",
        "                  'Electrical':  {'':0, 'NA':0, 'Mix':1, 'FuseP':2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5},\n",
        "                  'KitchenQual': {'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Functional':  {'Sal':0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7},\n",
        "                  'FireplaceQu': {'NA':0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex':5},\n",
        "                  'PoolQC':      {'NA':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex':4},\n",
        "                  'Fence':       {'NA':0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv':4},\n",
        "                  }\n",
        "\n",
        "  df_X = df[useful_fields].copy()\n",
        "  if use_dummies:\n",
        "    cols = [k for k in cleanup_nums.keys() if k in useful_fields]\n",
        "    df_X = pd.get_dummies(df_X, columns=cols)  # the categorical variables will be changed to one-hot encoding\n",
        "  else:\n",
        "    df_X.replace(cleanup_nums, inplace=True)  # convert continous categorial variables to numerical. Applicable only to\n",
        "\n",
        "  df_Y = df[target_field].copy()\n",
        "\n",
        "  x = df_X.to_numpy().astype(np.float32)\n",
        "  y = df_Y.to_numpy().astype(np.float32)\n",
        "\n",
        "  if price_max>0:\n",
        "    idxs = y<price_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  if area_max>0:\n",
        "    idxs = x[:,0]<area_max\n",
        "    x = x[idxs]\n",
        "    y = y[idxs]\n",
        "\n",
        "  return (x, y, df) if return_df else ((x, y, (df_X, df_Y)) if return_df_xy else (x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqWU0eHts1RM"
      },
      "source": [
        "# x, y, (df_X, df_Y) = house_prices_dataset(return_df_xy=True, use_dummies=True)\n",
        "# print(x.shape, y.shape)\n",
        "# df_X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtzVS-1Mxxe"
      },
      "source": [
        "# df_X.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWV7XpAJ3m56"
      },
      "source": [
        "x, y, (df_X, df_Y) = house_prices_dataset(return_df_xy=True, use_dummies=False)\n",
        "print(x.shape, y.shape)\n",
        "df_X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ahKST0Y3m6D"
      },
      "source": [
        "df_X.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Y.describe()"
      ],
      "metadata": {
        "id": "TIfqfsbd5Gwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91nj7znzMEpA"
      },
      "source": [
        "plt.plot(x[:, 0], y, '.')\n",
        "plt.xlabel('area, sq.ft')\n",
        "plt.ylabel('price, $');\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7CNxkPdNB4L"
      },
      "source": [
        "## 3. Blobs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8wXhleONKgZ"
      },
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKcmdcZf0VO8"
      },
      "source": [
        "x, y = make_blobs(n_samples=1000, centers=[[0,0], [5,5], [10, 0]])\n",
        "\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]  # affine transformation matrix\n",
        "x = np.dot(x, transformation)               # applied to point coordinated to make blobs less separable\n",
        "\n",
        "colors = \"ygr\"\n",
        "for i, color in enumerate(colors):\n",
        "    idx = y == i\n",
        "    plt.scatter(x[idx, 0], x[idx, 1], c=color, edgecolor='gray', s=25)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfbaOgfYNsq"
      },
      "source": [
        "## 4. Fashion MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgzzOS7YYTru"
      },
      "source": [
        "`Fashion-MNIST` is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (from https://github.com/zalandoresearch/fashion-mnist)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcV2gzmuYljJ"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the training and test examples has label (in fact - label index)  assigned to it:\n"
      ],
      "metadata": {
        "id": "872cjuKxYbe5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2LkoWfZEi4g"
      },
      "source": [
        "fmnist_class_names = ['T-shirt/top', 'Trouser', 'Pullover',\n",
        "                      'Dress', 'Coat', 'Sandal',\n",
        "                      'Shirt', 'Sneaker', 'Bag',\n",
        "                      'Ankle boot'\n",
        "                      ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, name in enumerate(fmnist_class_names):\n",
        "  print(f'{i}: {name}')"
      ],
      "metadata": {
        "id": "LHyZf-q2Yx7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPw6-GoPbT6U"
      },
      "source": [
        "Let's check few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHFd0sFHY4Li"
      },
      "source": [
        "n = 3\n",
        "fig, ax = plt.subplots(n, n, figsize=(2*n, 2*n))\n",
        "ax = [ax_xy for ax_y in ax for ax_xy in ax_y]\n",
        "for axi, im_idx in zip(ax, np.random.choice(len(train_images), n**2)):\n",
        "  im = train_images[im_idx]\n",
        "  im_class = train_labels[im_idx]\n",
        "  axi.imshow(im, cmap='gray')\n",
        "  axi.set_title(f'{im_class}: {fmnist_class_names[im_class]}', size=16)\n",
        "  axi.grid(False)\n",
        "  axi.axis('off')\n",
        "plt.tight_layout(pad=0,h_pad=0.8,w_pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Weather dataset"
      ],
      "metadata": {
        "id": "x2NWxK0BFwyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a weather time series [dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the Max Planck Institute for Biogeochemistry\n",
        "It contains weather reacord for 8 years of observation."
      ],
      "metadata": {
        "id": "LKsTkrMkGB7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather_df():\n",
        "  # inspired by https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "\n",
        "  # download and extract dataset\n",
        "  zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "  csv_path, _ = os.path.splitext(zip_path)\n",
        "\n",
        "  # load into pandas df\n",
        "  df = pd.read_csv(csv_path)\n",
        "\n",
        "  # dataset contains records every 10 min, we use hourly records only, thus\n",
        "  # slice [start:stop:step], starting from index 5 take every 6th record\n",
        "  df = df[5::6]\n",
        "\n",
        "  # replace errors in wind velocity to 0\n",
        "  wv = df['wv (m/s)']\n",
        "  bad_wv = wv == -9999.0\n",
        "  wv[bad_wv] = 0.0\n",
        "\n",
        "  max_wv = df['max. wv (m/s)']\n",
        "  bad_max_wv = max_wv == -9999.0\n",
        "  max_wv[bad_max_wv] = 0.0\n",
        "\n",
        "  # obtain timestamps from text time format\n",
        "  date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "  timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "  # genarate cyclic features for year and day\n",
        "  day = 24*60*60\n",
        "  year = (365.2425) * day\n",
        "\n",
        "  df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "  df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "  df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "  df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "D_bXLpwxGTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df = get_weather_df()"
      ],
      "metadata": {
        "id": "DsYDOZZKI7_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.head()"
      ],
      "metadata": {
        "id": "yQHfhrBMJA5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_df.describe().T"
      ],
      "metadata": {
        "id": "wPUvJcY3JjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(weather_df['Year cos'])\n",
        "# plt.plot(weather_df['Year sin'])"
      ],
      "metadata": {
        "id": "T7qIhgZuMGW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(weather_df['Day cos'][:7*24])\n",
        "# plt.plot(weather_df['Day sin'][:7*24])"
      ],
      "metadata": {
        "id": "WxziDnFwLqL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_future_T_dataset(X_len=24, Y_offset=48,\n",
        "                         X_features=['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
        "                         Y_features='T (degC)',\n",
        "                         standardize=True,\n",
        "                         oversample=10\n",
        "                         ):\n",
        "  \"\"\"\n",
        "  Generates pairs of input-label, using sequence of `X_len` samples as input\n",
        "  and value at offset `Y_offset` from start of this sequence as label.\n",
        "  Sample sequnces arte taken at random positions throughout the dataset.\n",
        "  Number of samples is obtained assuming non-overlaping wondows.\n",
        "  Oversampling factor allows to increase this value.\n",
        "\n",
        "  Args:\n",
        "    X_len (int): length of sample sequence\n",
        "    Y_offset (int): offset to the target value from the sequence start\n",
        "    X_features (list or str): features to be used as input\n",
        "    Y_features (list or str): features to be used as labels\n",
        "    standardize (Bool): flag whether to standardize the columns\n",
        "    oversample (int): increases number of samples by this factor wrt baseline\n",
        "                      n = len(df) // (Y_offset+1)\n",
        "  \"\"\"\n",
        "  weather_df = get_weather_df()\n",
        "\n",
        "  if standardize:\n",
        "    mean = weather_df.mean()\n",
        "    std = weather_df.std()\n",
        "    weather_df = (weather_df - mean) / std\n",
        "\n",
        "  df_X = weather_df[X_features]\n",
        "  df_Y = weather_df[Y_features]\n",
        "\n",
        "  n_records = len(df_X)\n",
        "  sample_len = Y_offset+1\n",
        "  n_samples = int((n_records-sample_len)//sample_len*oversample)\n",
        "  offsets = np.random.randint(0, n_records-sample_len, size=n_samples)\n",
        "  offsets.sort()\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  for o in offsets:\n",
        "    X.append(np.array(df_X[o:o+X_len]))\n",
        "    Y.append(np.array(df_Y[o+Y_offset:o+Y_offset+1]))\n",
        "\n",
        "  X = np.stack(X)\n",
        "  Y = np.concatenate(Y)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "tDQkIP4cJxuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = gen_future_T_dataset()"
      ],
      "metadata": {
        "id": "m-uVf1AOPLIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "m2f1pWLfPbJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f,fn in enumerate(['p (mbar)', 'T (degC)', 'rh (%)', 'wv (m/s)', 'wd (deg)', 'Day sin', 'Day cos', 'Year sin', 'Year cos']):\n",
        "  plt.figure(figsize=(5*(1+(f==1)), 4))\n",
        "  for s in range(10):\n",
        "    plt.plot(x[s, :, f])\n",
        "    if f==1:\n",
        "      plt.scatter(48, y[s], color=plt.gca().lines[-1].get_color())\n",
        "  plt.title(fn)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "JJU5MRbdRDEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHRXds9U9134"
      },
      "source": [
        "# `scikit-learn` interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2toQKrAzH_U"
      },
      "source": [
        "In this course we will primarily use the `scikit-learn` module.\n",
        "You can find extensive documentation with examples in the [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "\n",
        "The module contains A LOT of different machine learning methods, and here we will cover only few of them. What is great about `scikit-learn` is that it has a uniform and consistent interface.\n",
        "\n",
        "All the different ML approaches are implemented as classes with a set of same main methods:\n",
        "\n",
        "1. `fitter = ...`: Create fitter object.\n",
        "2. `fitter.fit(x, y[, sample_weight])`: Fit model to predict from list of smaples `x` a list of target values `y`.\n",
        "3. `y_pred = fitter.predict(X)`: Predict using the trained model.\n",
        "4. `s = fitter.score(x, y[, sample_weight])`: Obtain a relevant performance measure of the trained model.\n",
        "\n",
        "This allows one to easily replace one approach with another and find the best one for the problem at hand, by simply using a regression/classification object of another class, while the rest of the code can remain the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLR5-eQ2vtz"
      },
      "source": [
        "It is useful to know that generally in scikit-learn the input data is represented as a matrix $X$ of dimensions `n_samples x n_features` , whereas the supervised labels/values are stored in a matrix $Y$ of dimensions `n_samples x n_target` ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Regression and Classification"
      ],
      "metadata": {
        "id": "HZ5LFPmmp1Gb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4qgOdz7Yyeb"
      },
      "source": [
        "## 1.Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh6lII-Hz8u-"
      },
      "source": [
        "In many cases the scalar value of interest - dependent variable - is (or can be approximated as) linear combination of the independent variables.\n",
        "\n",
        "In linear regression the estimator is searched in the form: $$\\hat{y}(\\bar{x} | \\bar{w}) = w_0 + w_1 x_1 + ... + w_p x_p$$\n",
        "\n",
        "The parameters $\\bar{w} = (w_1,..., w_p)$ and $w_0$ are designated as `coef_` and `intercept_` in `sklearn`.\n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/linear_model.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlf6_berQ1vq"
      },
      "source": [
        "### 1. Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatxRr8bOuTs"
      },
      "source": [
        "[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients $\\bar{w} = (w_1,..., w_p)$ and $w_0$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
        "\n",
        "Mathematically it solves a problem of the form:  $$\\bar{w} = \\min_{w_i} || \\bar{X} \\cdot \\bar{w} - y||_2^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqh7XwGkNg6r"
      },
      "source": [
        "x, y = get_linear(n_d=1, sigma=3, n_points=30)  # p==1, 1D input\n",
        "plt.scatter(x, y);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFawJfQJOKX3"
      },
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diHNLTNMOek5"
      },
      "source": [
        "w, w0 = reg.coef_, reg.intercept_\n",
        "print(w, w0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyeHY3bxPYSF"
      },
      "source": [
        "plt.scatter(x, y, marker='*', label='data points')\n",
        "x_f = np.linspace(x.min(), x.max(), 10)\n",
        "y_f = w0 + w[0] * x_f\n",
        "plt.plot(x_f, y_f, label='fit', c='r')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNX-5gYOIi40"
      },
      "source": [
        "# rmse\n",
        "np.std(y - reg.predict(x))  # or use metrics.mean_squared_error(..., squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID0Hdzx0NvxF"
      },
      "source": [
        "# R2\n",
        "reg.score(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rg2_DZCHgJE"
      },
      "source": [
        "Let's try 2D input.\n",
        "Additionally, here we will split the whole dataset into training and test subsets using the `train_test_split` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK5MILosSI7d"
      },
      "source": [
        "n_d = 2\n",
        "x, y = get_linear(n_d=n_d, n_points=1000, sigma=5)\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x_train[:,0], x_train[:,1], y_train, marker='x', s=40)\n",
        "ax.scatter(x_test[:,0], x_test[:,1], y_test, marker='+', s=80)\n",
        "\n",
        "xx0 = np.linspace(x[:,0].min(), x[:,0].max(), 10)\n",
        "xx1 = np.linspace(x[:,1].min(), x[:,1].max(), 10)\n",
        "xx0, xx1 = [a.flatten() for a in np.meshgrid(xx0, xx1)]\n",
        "xx = np.stack((xx0, xx1), axis=-1)\n",
        "yy = reg.predict(xx)\n",
        "ax.plot_trisurf(xx0, xx1, yy, alpha=0.25, linewidth=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW5GiLlhS3Y8"
      },
      "source": [
        "# rmse\n",
        "print('train rmse =', np.std(y_train - reg.predict(x_train)))\n",
        "print('test rmse =', np.std(y_test - reg.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Fb1zb5S3ZG"
      },
      "source": [
        "# R2\n",
        "print('train R2 =', reg.score(x_train, y_train))\n",
        "print('test R2 =', reg.score(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6s2Amob48j"
      },
      "source": [
        "### EXERCISE 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRi8SPiMb9FM"
      },
      "source": [
        "Use linear regression to fit house prices dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaQVHyvPcHW2"
      },
      "source": [
        "# 1. make train/test split\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate RMSE, MAD, and R2 on train and test datasets\n",
        "\n",
        "# 4. plot y vs predicted y for test and train parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZX9MQlORLfY"
      },
      "source": [
        "### 2. Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRUwQD5UR0Vf"
      },
      "source": [
        "Logistic regression, despite its name, is a linear model for classification rather than regression. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
        "\n",
        "In binary logistic regression the probability $p$ of a point belonging to a class is modeled as: $$\\frac{p}{1-p} = e^{w_0 + w_1 x_1 + ... + w_p x_p}$$\n",
        "\n",
        "\\\\\n",
        "In case of $N$ classes 2 options are available:\n",
        "* One-versus-rest (`ovr`) performs $N$ independent binary logistic regressions for each class $i$ just as for the binary case: $$\\frac{p_i}{1-p_i} = e^{w_{i0} + w_{i1} x_1 + ... + w_{ip} x_p}$$\n",
        "Final probability is obtained by evaluating the probability for all classes, and then normalizing it: $$\\hat p_i = \\frac{p_i}{\\sum_j{p_j}}$$\n",
        "\n",
        "\\\\\n",
        "* Multinomial (`multinomial`) logistic regression fits probability for each class $i$ with respect to a pivot, e.g. class $0$:\n",
        "$$\\frac{p_i}{p_0} = e^{w_{i0} + w_{i1} x_1 + ... + w_{ip} x_p},\\, i\\neq0$$\n",
        "Then SoftMax function is used to find the predicted probability of each class.\n",
        "\n",
        "\\\\\n",
        "The binary class $\\ell_2$-penalized logistic regression minimizes the following cost function:\n",
        "$$\\min_{w, c}  \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) + \\lambda \\frac{1}{2}w^T w$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnNRNDj-zbE"
      },
      "source": [
        "# routine for coloring 2d space according to class prediction\n",
        "\n",
        "def plot_prediction_2d(x_min, x_max, y_min, y_max, classifier, ax=None):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .02  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "\n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(),\n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "\n",
        "  # obtain class for each node\n",
        "  mesh_nodes_class = classifier.predict(mesh_xy_coords)\n",
        "\n",
        "  # reshape to the shape (n_mesh_rows, n_mesh_cols)==mesh_nodes_x.shape for visualization\n",
        "  mesh_nodes_class = mesh_nodes_class.reshape(mesh_nodes_x.shape)\n",
        "\n",
        "  # Put the result into a color countour plot\n",
        "  ax = ax or plt.gca()\n",
        "  ax.contourf(mesh_nodes_x,\n",
        "              mesh_nodes_y,\n",
        "              mesh_nodes_class,\n",
        "              cmap='Pastel1', alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# routine for plotting 3d probability hyperplanes\n",
        "\n",
        "def plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier):\n",
        "  \"\"\"\n",
        "  Creates 2D mesh, predicts class for each point on the mesh, and visualises it\n",
        "  \"\"\"\n",
        "\n",
        "  mesh_step = .5  # step size in the mesh\n",
        "  x_coords = np.arange(x_min, x_max, mesh_step) # coordinates of mesh colums\n",
        "  y_coords = np.arange(y_min, y_max, mesh_step) # coordinates of mesh rows\n",
        "\n",
        "  # create mesh, and get x and y coordinates of each point point\n",
        "  # arrenged as array of shape (n_mesh_rows, n_mesh_cols)\n",
        "  mesh_nodes_x, mesh_nodes_y = np.meshgrid(x_coords, y_coords)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "  # prepare xy pairs for prediction: matrix of size (n_mesh_rows*n_mesh_cols, 2)\n",
        "  mesh_xy_coords = np.stack([mesh_nodes_x.flatten(),\n",
        "                             mesh_nodes_y.flatten()], axis=-1)\n",
        "\n",
        "  # obtain class for each node\n",
        "  coef = classifier.coef_\n",
        "  intercept = classifier.intercept_\n",
        "\n",
        "  df_dict = {\n",
        "      'x':[],\n",
        "      'y':[],\n",
        "      'p':[],\n",
        "      'c':[]\n",
        "  }\n",
        "  def fill_dict(x, y, p, c):\n",
        "    df_dict['x'].append(x)\n",
        "    df_dict['y'].append(y)\n",
        "    df_dict['p'].append(p)\n",
        "    df_dict['c'].append(c)\n",
        "\n",
        "  for c in classifier.classes_:\n",
        "    coef_c = coef[c]\n",
        "    intercept_c = intercept[c]\n",
        "    for x, y in mesh_xy_coords:\n",
        "      p = x*coef_c[0] + y*coef_c[1] + intercept_c\n",
        "      fill_dict(x, y, p, c)\n",
        "  df = pd.DataFrame(df_dict)\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='p', color='c', size_max=1)\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "8-GFZDsykukn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJHWawkq0ev"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "x, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "x = np.dot(x, transformation)\n",
        "\n",
        "multi_class = 'multinomial'\n",
        "# do fit\n",
        "clf = linear_model.LogisticRegression(solver='sag', max_iter=100,\n",
        "                          multi_class=multi_class, )\n",
        "clf.fit(x, y)\n",
        "\n",
        "# print the training scores\n",
        "print(\"training accuracy : %.3f (%s)\" % (clf.score(x, y), multi_class))\n",
        "\n",
        "# get range for visualization\n",
        "x_0 = x[:, 0]\n",
        "x_1 = x[:, 1]\n",
        "x_min = x_0.min() - 1\n",
        "x_max = x_0.max() + 1\n",
        "y_min = x_1.min() - 1\n",
        "y_max = x_1.max() + 1\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=clf)\n",
        "\n",
        "plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
        "plt.axis('tight')\n",
        "\n",
        "# Plot also the training points\n",
        "colors = 'rbg'\n",
        "\n",
        "for i, color in zip(clf.classes_, colors):\n",
        "    idx = np.where(y == i)\n",
        "    plt.scatter(x_0[idx], x_1[idx], c=color, cmap=plt.cm.Paired,\n",
        "                edgecolor='gray', s=30, linewidth=0.2)\n",
        "\n",
        "# Plot the three one-against-all classifiers\n",
        "coef = clf.coef_\n",
        "intercept = clf.intercept_\n",
        "\n",
        "def plot_hyperplane(c, color):\n",
        "    def line(x0):\n",
        "        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
        "    plt.plot([x_min, x_max], [line(x_min), line(x_max)],\n",
        "              ls=\"--\", color=color)\n",
        "\n",
        "for i, color in zip(clf.classes_, colors):\n",
        "    plot_hyperplane(i, color)\n",
        "\n",
        "plt.show()\n",
        "#plot_hyperplanes_3d(x_min, x_max, y_min, y_max, classifier=clf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ69XKdbZcA3"
      },
      "source": [
        "### EXERCISE 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__9jcqXzZaQp"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRBuV1m_E4Ll"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304Ul40adUT2"
      },
      "source": [
        "We will reshape 2-d images to 1-d arrays for use in scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtD8C8_4a7dP"
      },
      "source": [
        "n_train = len(train_labels)\n",
        "x_train = train_images.reshape((n_train, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJj7ofWD_Wp2"
      },
      "source": [
        "Now use a multinomial logistic regression classifier, and measure the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeIKcMeV_rmk"
      },
      "source": [
        "# 1. Create classifier\n",
        "\n",
        "# 2. fit the model\n",
        "\n",
        "# 3. evaluate accuracy on train and test datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-CGSS2OZKHD"
      },
      "source": [
        "## 2. Trees & Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxtv48o-F1Ku"
      },
      "source": [
        "### 1. Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l582Sr0_WGXj"
      },
      "source": [
        "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning **simple** decision rules inferred from the data features.\n",
        "\n",
        "They are fast to train, easily interpretable, capture non-linear dependencies, and require small amount of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5raIG_WQfg"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "dtcs = []\n",
        "for depth in (1, 2, 3, 4):\n",
        "    # do fit\n",
        "    dtc = tree.DecisionTreeClassifier(max_depth=depth, criterion='gini')  # 'entropy'\n",
        "    dtcs.append(dtc)\n",
        "    dtc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (depth=%d)\" % (dtc.score(X, y), depth))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2,  figsize=(14,7), dpi=300)\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=dtc, ax=ax[0])\n",
        "\n",
        "    ax[0].set_title(\"Decision surface of DTC (%d)\" % depth)\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = \"rbg\"\n",
        "    for i, color in zip(dtc.classes_, colors):\n",
        "        idx = np.where(y == i)\n",
        "        ax[0].scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    with plt.style.context('classic'):\n",
        "      tree.plot_tree(dtc, ax=ax[1]);\n",
        "\n",
        "    plt.tight_layout(pad=0.5, h_pad=0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IgYhWghyUhL"
      },
      "source": [
        "Given fraction of samples of class $i$ as $p_i$\n",
        "\n",
        "* the Gini index is:\n",
        "$$G = 1 - \\sum_i {p_i^2}, $$\n",
        "\n",
        "* the Entropy is:\n",
        "$$E =  - \\sum_i {p_i * log(p_i)}, $$\n",
        "\n",
        "The optimization is performed in a greedy manner, one split at a time, minimizing impurity in the descendant nodes $G=G_1+G_2$ when the Gini index is used, or maximizing informatin gain: $IG = E_{node} - E_1 - E_2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYpitlKczjki"
      },
      "source": [
        "text_representation = tree.export_text(dtcs[2])\n",
        "print(text_representation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsuFUM2nlP5"
      },
      "source": [
        "for i, dtc in enumerate(dtcs):\n",
        "  viz_model = dtreeviz.model(dtc,\n",
        "                           X_train=X, y_train=y,\n",
        "                           feature_names=['x[0]', 'x[1]'])\n",
        "\n",
        "  v = viz_model.view()     # render as SVG into internal object\n",
        "  fn = f'tree_{i}.svg'\n",
        "  v.save(fn)\n",
        "  display(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, dtc in enumerate(dtcs):\n",
        "  fn = f'tree_{i}.svg'\n",
        "  display(SVG(fn))"
      ],
      "metadata": {
        "id": "YL11Mw0yjemc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJK1XADt94uK"
      },
      "source": [
        "Additionally we can directly inspect relevance of the input features for the classification (impurity based):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxEFpn2P9Uek"
      },
      "source": [
        "plt.plot(dtcs[2].feature_importances_, '.')\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHZ-hHGuY5aG"
      },
      "source": [
        "### 2. Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zETTKyFmwTae"
      },
      "source": [
        "The `sklearn.ensemble` provides several ensemble algorithms. RandomForest is an averaging algorithm based on randomized decision trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n",
        "\n",
        "Individual decision trees typically exhibit high variance and tend to overfit.\n",
        "In random forests:\n",
        "* each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
        "* when splitting each node during the construction of a tree, the best split is found from a random subset of features, according to `max_features` parameter.\n",
        "\n",
        "The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant, hence yielding an overall better model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4hfPUqSZCbH"
      },
      "source": [
        "# make 3-class dataset for classification\n",
        "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
        "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
        "transformation = [[0.4, 0.2], [-0.4, 1.2]]\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "for n_est in (1, 4, 50):\n",
        "    # do fit\n",
        "    rfc = ensemble.RandomForestClassifier(max_depth=4, n_estimators=n_est,)\n",
        "    rfc.fit(X, y)\n",
        "\n",
        "    # print the training scores\n",
        "    print(\"training score : %.3f (n_est=%d)\" % (rfc.score(X, y), n_est))\n",
        "\n",
        "    # get range for visualization\n",
        "    x_0 = X[:, 0]\n",
        "    x_1 = X[:, 1]\n",
        "    x_min = x_0.min() - 1\n",
        "    x_max = x_0.max() + 1\n",
        "    y_min = x_1.min() - 1\n",
        "    y_max = x_1.max() + 1\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plot_prediction_2d(x_min, x_max, y_min, y_max, classifier=rfc)\n",
        "\n",
        "    # Plot also the training points\n",
        "    colors = 'rbg'\n",
        "    for i, color in enumerate(colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(x_0[idx], x_1[idx], c=color,\n",
        "                    edgecolor='black', s=20, linewidth=0.2)\n",
        "\n",
        "    # Plot the three one-against-all classifiers\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY6EVEestysr"
      },
      "source": [
        "plt.figure(dpi=300)\n",
        "with plt.style.context('classic'):\n",
        "  tree.plot_tree(rfc.estimators_[20]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_model = dtreeviz.model(rfc.estimators_[20], X, y, feature_names=[f'{i}' for i in np.arange(len(X[0]))])\n",
        "\n",
        "v = viz_model.view()     # render as SVG into internal object\n",
        "display(v)"
      ],
      "metadata": {
        "id": "nNvlsWBC7f2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLruRG-g_pyD"
      },
      "source": [
        "For a forest we can also evaluate the feature importance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks42i4mq-RM-"
      },
      "source": [
        "importances = np.array([e.feature_importances_ for e in rfc.estimators_])\n",
        "\n",
        "#plt.plot(importances.T, '.')\n",
        "plt.bar(['0', '1'], importances.mean(axis=0), yerr=importances.std(axis=0))\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTNL6_57A0rq"
      },
      "source": [
        "Alternatively we can use permutation to study feature importance.\n",
        "It evaluates decrease in performance (model's `.score` or specified in `scoring` parameter) for each variable when its values are shuffled between samples.\n",
        "It can be time-consuming as requires re-evaluation for each feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrD0XYtI_8mQ"
      },
      "source": [
        "p_importances = permutation_importance(rfc, X, y, n_repeats=10, n_jobs=-1)\n",
        "\n",
        "plt.bar(['0', '1'],\n",
        "        p_importances.importances_mean,\n",
        "        yerr=p_importances.importances_std)\n",
        "plt.xlabel('feature')\n",
        "plt.ylabel('importance')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhycm2S6wbz"
      },
      "source": [
        "### EXERCISE 3 : Random Forest classifier for FMNIST\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I20uBLiMoURH"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "n = len(train_labels)\n",
        "x_train = train_images.reshape((n, -1))\n",
        "y_train = train_labels\n",
        "\n",
        "n_test = len(test_labels)\n",
        "x_test = test_images.reshape((n_test, -1))\n",
        "y_test = test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd59TNBWfHgX"
      },
      "source": [
        "Classify fashion MNIST images with Random Forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8waJCx33peIG"
      },
      "source": [
        "# 1. Create classifier. As the number of features is big (784), use bigger tree\n",
        "# depth (max_depth parameter), try in range 10-500.\n",
        "\n",
        "# 2. What is the maximum number of leaves in tree of depth n?\n",
        "# To reduce variance we should avoid leaves with too litle samples. You could\n",
        "# limit the total number of tree leaves (max_leaf_nodes parameter) to 10-1000.\n",
        "# Alternatively you can use min_samples_split & min_samples_leaf\n",
        "\n",
        "# 3. Try different number of estimators (n_estimators)\n",
        "\n",
        "# 4. Fit the model\n",
        "\n",
        "# 5. Inspect training and test accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dG6U6s3T95t"
      },
      "source": [
        "### EXERCISE 4: Random Forest regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlypA1MaT95u"
      },
      "source": [
        "X, y, (df_x, df_y) = house_prices_dataset(return_df_xy=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYTwkoSpT95u"
      },
      "source": [
        "Predict the house prices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4wobzB_T95u"
      },
      "source": [
        "# 1. Create regressor. (ensemble.RandomForestRegressor)\n",
        "# Try different number of estimators (n_estimators)\n",
        "\n",
        "# 2. Fit the model\n",
        "\n",
        "# 3. Inspect training and test accuracy\n",
        "\n",
        "# 4. Try to improve performance by adjusting hyperparameters.\n",
        "# How does it compare to linear model?\n",
        "\n",
        "# 5. Use dtreeviz to visualize a tree from the ensamble\n",
        "\n",
        "# 6. Study the feature importance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clustering"
      ],
      "metadata": {
        "id": "jFnEt7Lbr4_4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09xX6mfGYBHH"
      },
      "source": [
        "Unsupervised learning technique different from supervised ones from the fact that data are not labelled.\n",
        "\n",
        "We do not aim at fitting a mapping from $X$ to $y$, but to understand pattern in the data cloud $X$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUus-6PW95xA"
      },
      "source": [
        "### 1. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "T0sSzs5h95xC"
      },
      "source": [
        "#### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "XJj9Ry6M95xF"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "Clustering techniques divide the set of data into group of atoms having common features. Each data point $p$ gets assigned a label $l_p \\in \\{1,..,K\\}$. In this presentation the data points are supposed to have $D$ features, i.e. each data point belongs to $\\mathbf{R}^D$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "CQTkkdLnefft"
      },
      "source": [
        "**Terminology and output of a K-means computation:**\n",
        "- *Within-cluster variation* : $L_k$ is called within cluster variation.\n",
        "\n",
        "- *Silhouette score*: K-means clustering fixes the number of clusters a priori. Some technique must be chosen to score the different optimal clusterings for different $k$. One technique chooses the best *Silouhette score*. Intuitively, this evaluates the typical distance of points within a same clusters and compares it against the typical distance of points belonging to neighboring but different clusters ( https://en.wikipedia.org/wiki/Silhouette_(clustering) )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "gOmoafxD95xT"
      },
      "source": [
        "#### Sklearn: implementation and usage of K-means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "bxacfstD95xW"
      },
      "source": [
        "We start with a 2D example that can be visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "osqtrCM295xn"
      },
      "source": [
        "First we load the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "C6X5O32r95xp"
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_BTlSM4W95x0"
      },
      "source": [
        "Explore the data-set checking the dataset dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "LsW9d6x_95x3"
      },
      "outputs": [],
      "source": [
        "sh = points.shape\n",
        "print(sh)\n",
        "n_p, n_f = sh\n",
        "print(f'We have {n_p} points with {n_f} features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "usXcr6XP95yD"
      },
      "outputs": [],
      "source": [
        "plt.plot(points[:,0], points[:,1], 'o')\n",
        "plt.xlabel('feature-1')\n",
        "plt.ylabel('feature-2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "_kLezqgz95yQ"
      },
      "source": [
        "It looks visually that the data set has three clusters. We will cluster them using K-means. As usual, we create a KMeans object. Note that we do not need to initialize it with a data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "5cjLh0ZX95yS"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=3, random_state=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "fHf2oH7p95yf"
      },
      "source": [
        "A call to the fit method computes the cluster centers which can be plotted alongside the data-set. They are accessible from the cluster_centers_ attribute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ET3C5t-r95yh"
      },
      "outputs": [],
      "source": [
        "clusterer.fit(points)\n",
        "\n",
        "plt.plot(points[:,0], points[:,1],'o')\n",
        "\n",
        "plt.plot(clusterer.cluster_centers_[:,0],clusterer.cluster_centers_[:,1],'o',markersize=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "z4ou8qEVLvWy"
      },
      "outputs": [],
      "source": [
        "clusterer.cluster_centers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "J7H4hw4l95yu"
      },
      "source": [
        "The predict method assigns a new point to the nearest cluster. We can use predict with the training dataset and color the data-set according to the cluster label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "BqPcVkcR95yw"
      },
      "outputs": [],
      "source": [
        "cluster_labels=clusterer.predict(points)\n",
        "print(cluster_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(points[:,0], points[:,1], c=cluster_labels, cmap='jet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c4St8T1b0j7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wBtdsqxK95y6"
      },
      "source": [
        "Finally, we can try to vary the number of clusters and score them with the Silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "_dg8v1ST95y8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sil=[]\n",
        "\n",
        "for iclust in range(2,6):\n",
        "    clusterer = KMeans(n_clusters=iclust, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(points)\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    sil.append(score)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels, cmap='jet')\n",
        "    plt.show()\n",
        "\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.plot(np.arange(len(sil))+2, sil,'-o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "8RVDOdZB95zI"
      },
      "source": [
        "The same techniques can be used on high dimensional data-sets. We use here the famous MNIST dataset for integer digits, that we are downloading from tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "RSzh1J9q95zJ"
      },
      "outputs": [],
      "source": [
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "X=train_images[:5000,:].reshape(5000,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FTguyfEH95zV"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "image=X[1232,:].reshape(28,28)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.grid(False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "08EWsPIR95zh"
      },
      "source": [
        "We can cluster the images exactly as we did for the 2d dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "-TU70SxP95zm"
      },
      "outputs": [],
      "source": [
        "clusterer = KMeans(n_clusters=10, random_state=10)\n",
        "cluster_labels = clusterer.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "Q2QNHiFt95zx"
      },
      "source": [
        "We can plot the cluster centers (which are 2D figures!) to see if the clustering is learning correct patterns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "ba_LXoum95z0"
      },
      "outputs": [],
      "source": [
        "for iclust in range(10):\n",
        "    plt.imshow(clusterer.cluster_centers_[iclust].reshape(28,28), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "1QTiCQE395z9"
      },
      "source": [
        "You can see that the model looks to assign one class to the same good. Nevertheless, using the cluster centers and with a further trick, in exercise 2 you will build a digit recognition model !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "jInWo3Uk95z_"
      },
      "source": [
        "#### EXERCISE 1: Discover the number of Gaussians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "FZSVE8wfeffz"
      },
      "outputs": [],
      "source": [
        "### In this exercise you are given the dataset points, consisting of high-dimensional data. It was built taking random\n",
        "# samples from a number k of multimensional gaussians. The data is therefore made of k clusters but, being\n",
        "# very high dimensional, you cannot visualize it. Your task it to use K-means combined with the Silouhette\n",
        "# score to find the number of k.\n",
        "\n",
        "# 1. Load the data using the function points=load_ex1_data_clust() , check the dimensionality of the data.\n",
        "\n",
        "points= ...\n",
        "npoints, ndims = ...\n",
        "print(npoints,ndims)\n",
        "\n",
        "# 2. Fix the number of clusters k and define a KMeans clusterer object. Perform the fitting and compute the Silhouette score.\n",
        "# Save the results on a list.\n",
        "res= ...\n",
        "for n_clusters in ... :\n",
        "    clusterer = ...\n",
        "    cluster_labels = ...\n",
        "    score= ...\n",
        "    res.append(score)\n",
        "\n",
        "# 3. Plot the Silhouette scores as a function ok k? What is the number of clusters ?\n",
        "plt.plot(...,...,'-o')\n",
        "\n",
        "# 4. Optional. Check the result that you found via umap. Remember the syntax umap_model=umap.UMAP(random_state=xxx) to\n",
        "# istantiate the umap model and than use fit_transform to compute the coordinate with the reduced dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "v9TSuObo950Z"
      },
      "source": [
        "#### EXERCISE 2: Predict the garment using K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "hidden": true,
        "id": "PSyJzomm950e"
      },
      "source": [
        "###### DESCRIPTION ###############\n",
        "\n",
        "In this exercise you are asked to use the clustering performed by K-means to predict the good in the F-mnist dataset. We show here how clustering can be used as a preprocessing tool for a supervised task !\n",
        "\n",
        "We will follow the pipeline to fit the model :\n",
        "\n",
        "1- We perform K-means clustering using just the input data and fixing for the start the number of clusters to 10 ;\n",
        "\n",
        "2- To each cluster, we will attach a label, finding the most represented good inside that cluster. Let's call that label\n",
        " assignment[c] for cluster c ;  \n",
        "\n",
        "When using the model for prediction of a new image we will :\n",
        "\n",
        "1- Find the cluster center nearest to the new image ;\n",
        "\n",
        "2- Assign the new image to the good most represented in that cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4JuuvrRIeff0"
      },
      "outputs": [],
      "source": [
        "# Follow the following STEPS to solve the exercise\n",
        "\n",
        "# STEP 1. Load the dataset and reshape it accordingly.\n",
        "fmnist = ...\n",
        "(train_images, train_labels), (test_images, test_labels) = ...\n",
        "X_train=train_images[:5000,:].reshape(...,...)\n",
        "y_train=train_labels[:5000]\n",
        "X_test=test_images[:1000,:].reshape(...,...)\n",
        "y_test=test_labels[:1000]\n",
        "\n",
        "\n",
        "# STEP 2.\n",
        "# Define the cluster KMeans object and fit the model on the training set.\n",
        "clusterer = ...\n",
        "clusterer.fit(...)\n",
        "\n",
        "# STEP 3.\n",
        "# Compute the cluster labels.\n",
        "cluster_labels=...\n",
        "\n",
        "# STEP 4.\n",
        "# Compute the assignment list. assignment[i] will be the majority class of the i-cluster\n",
        "# You can use, if you want,  the function most_common with arguments (k,y_train, cluster_labels)\n",
        "# this compute the assignment list. Print the assignment list to explore its values.\n",
        "\n",
        "def most_common(nclusters, supervised_labels, cluster_labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    - nclusters : the number of clusters\n",
        "    - supervised_labels : for each garment, the labelling provided by the training data ( e.g. in y_train or y_test)\n",
        "    - cluster_labels : for each garment, the cluster it was assigned by K-Means using the predict method of the Kmeans object\n",
        "\n",
        "    Returns:\n",
        "    - a list \"assignment\" of lengths nclusters, where assignment[i] is the majority class of the i-cluster\n",
        "    \"\"\"\n",
        "    assignment=[]\n",
        "    for icluster in range(nclusters):\n",
        "        indices=list(supervised_labels[cluster_labels==icluster])\n",
        "        try:\n",
        "            chosen= max(set(indices), key=indices.count)\n",
        "        except ValueError :\n",
        "            print('Em')\n",
        "            chosen=1\n",
        "        assignment.append(chosen)\n",
        "    return assignment\n",
        "assignment=...\n",
        "\n",
        "# STEP 5.\n",
        "# Predict the cluster labels for the test set\n",
        "cluster_labels_test=...\n",
        "\n",
        "# STEP 6.\n",
        "# using the cluster labels predicted in STEP 5 and the previously computed assignment[] list,\n",
        "# predict what are according to your model the predicted goods for the test set, call them new_labels\n",
        "# (The Python notation suggested is called list_comprehension)\n",
        "new_labels = [assignment[...] for ... in ... ]\n",
        "\n",
        "# STEP 7.\n",
        "# Using  a call cm=metrics.confusion_matrix( y_test, new_labels ) you can print the confusion matrix on the test set,\n",
        "# which\n",
        "# provides information on the quality of the fit. Print the percentage of correctly classified examples.\n",
        "# For example, you can divide the sum of the elements on the diagonal of cm (save it in denominator \"den\")\n",
        "# and divide by the sum of all entries of cm (save it in numerator \"num\").\n",
        "cm= ...\n",
        "print(cm)\n",
        "num=...\n",
        "den=...\n",
        "accuracy=num/den\n",
        "print(num/den)\n",
        "\n",
        "#  Perform again steps 2 / 7 increasing the number of clusters from 10 to 40 what happens to the performance ?\n",
        "for iclust in ... :\n",
        "    print(f'Test set with {iclust} clusters')\n",
        "    # BEGIN OF YOUR CODE .\n",
        "    # ...\n",
        "    # END OF YOUR CODE\n",
        "    print(assignment)\n",
        "    print(accuracy)\n",
        "    print(cm)\n",
        "    return traj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "b3VoJKsVeff0"
      },
      "source": [
        "### 2. Hierarchical clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PfeYW8eKeff0"
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "In hierarchical clustering we do not have only an optimal set of clusters, but for each different \"length scale\" we have a different set of clusters.\n",
        "\n",
        "**Algorithm:**\n",
        "\n",
        "- We start with a length scale $l=0$ at the beginning and consider all sample elements as different clusters.\n",
        "\n",
        "- We increase than $l$ to values larger than zero. Let's call the minimum distance between pair of points $l_0$. As soon as we reach $l=l_0$, these elements are merged into a new cluster (greedy strategy).\n",
        "\n",
        "To proceed further we need to define a distance between subsets $S_1,S_2$ of points. In the \"single linkage\" flavour we define:\n",
        "\n",
        "$$d(S_1,S_2)=min_{a\\in S_1, b\\in S_2} d(a,b)$$\n",
        "\n",
        "- This way we can proceed increasing $l>l_0$. As soon as we find two clusters with distance smaller than $l_1>l_0$, we merge them.\n",
        "\n",
        "- We keep on increasing $l$ as far as one one cluster remains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "yDf_mhqdeff0"
      },
      "source": [
        "The result of this clustering procedure can be summarized in a `dendrogram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "xhNhBihUeff0"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.cluster import hierarchy\n",
        "from ipywidgets import interact\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "points=km_load_th1()\n",
        "Z = hierarchy.linkage(points, 'single')\n",
        "n_clusters=int(np.max(Z[:,[0,1]].flatten()))\n",
        "print(n_clusters)\n",
        "\n",
        "@interact\n",
        "def plot(t=(0,7,0.1)):\n",
        "    fig, axes=plt.subplots(1,2,figsize=(15,10), gridspec_kw={'width_ratios': [1, 2]})\n",
        "    fl = fcluster(Z,t=t,criterion='distance')\n",
        "    maps={}\n",
        "    for clust in range(n_clusters):\n",
        "        maps[clust]=(fl==clust)\n",
        "    c=0\n",
        "    for clust in range(n_clusters):\n",
        "        if np.sum(maps[clust])>0:\n",
        "            c+=1\n",
        "    d = hierarchy.dendrogram(Z,ax=axes[1], color_threshold=t)\n",
        "    axes[1].axhline(t,linestyle='--',color='red')\n",
        "    axes[1].set_ylabel('Cluster distance')\n",
        "    axes[1].axes.get_xaxis().set_visible(False)\n",
        "    axes[1].set_xlabel('Points')\n",
        "    axes[1].set_title(f'Number clusters: {c}')\n",
        "    axes[0].scatter(points[d['leaves'],0],points[d['leaves'],1], color=d['leaves_color_list'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4lTUxeSmeff1"
      },
      "source": [
        "**Properties:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "O98E0rxHeff1"
      },
      "source": [
        "If we fix a certain cutoff length $l_c$, the clusters identified $C_1,...,C_n$ at that length are such that :\n",
        "\n",
        "1- The constintute a disjoint partition of the whole dataset, i.e. they are mutually non intersecting aand each point belongs to a cluster\n",
        "\n",
        "2- The distance between two clusters $d(C_1,C_j)$ is larger than $l_c$ for each $i,j$.\n",
        "\n",
        "( Check of point 2: If it was smaller, take $A,B$ such that $d(A,B)=d(C_1,C_j)=l'<l_c$ and $A \\in C_1$, $B \\in C_2$. But then, when merging clusters at length scale $l'$ the clusters at which $A,B$ belonged to, would have been merged. By construction, after two points are merged into the same cluster at a length scale, they belong to the same cluster at all larger length scales. This is a contradiction. )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9WGjVQ951B"
      },
      "source": [
        "### 3. Gaussian mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "baYX-IKOB461"
      },
      "source": [
        "#### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "jkueVtkf951D"
      },
      "source": [
        "K-Means is a modelling procedure which is biased towards clusters of circular shape and therefore does not always work perfectly, as the following examples show:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "S_kqTtgi951F"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clusterer = KMeans(n_clusters=3, random_state=10, )\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "bRsc4ZOV951O"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clusterer = KMeans(n_clusters=2, random_state=10)\n",
        "cluster_labels=clusterer.fit_predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.title('K-Means')\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "4SvVyxXu952E"
      },
      "source": [
        "A Gaussian mixture model is able to fit these kinds of clusters. In a Gaussian mixture model, each data-set is supposed to be a random point from the distribution:\n",
        "$$f(\\mathbf{x})=\\sum_c \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$$\n",
        ", which is called a Gaussian mixture (N stands for Normal distribution). The parameters $\\{\\pi_c,\\mathbf{\\mu_c},\\mathbf{\\Sigma_c}\\}$ are fitted from the data using a minimization procedure (maximum likelihood via the EM algorithm) and $N_c$ is the chosen number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "oOalY-1veff2"
      },
      "source": [
        "This density has the following interpretation. Suppose each data point $\\hat{X}$ is a random variable resulting from the following 2-step procedure:\n",
        "\n",
        "1- First a random variable $\\hat {C}$ is extracted, with values in $\\{1,...,N_c\\}$ and probabilities $\\{\\pi_1,...\\pi_{N_c}\\}$. This random variable fixes a cluster.\n",
        "\n",
        "2- When we know $\\hat {C}=c$ we extract $\\hat{X}$ according to the density $N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x})$.\n",
        "\n",
        "This procedure is summarized by the formulas:\n",
        "\n",
        "$$p(\\hat{C}=c)=\\pi_c$$\n",
        "\n",
        "$$p(\\hat{X}=\\mathbf{x}|\\hat{C}=c)=N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c})(\\mathbf{x})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "wqOLzPso952G"
      },
      "source": [
        "**Output of a GM computation:**\n",
        "- *Cluster probabilities:* A gaussian mixtures model is an example of soft clustering, where each data point $p$ does not get assigned a unique cluster, but a distribution over clusters $f_p(c), c=1,...,N_c$.\n",
        "\n",
        "Given the fitted parameters,  $f_p(c)$ is computed as: $$f_p(c)=\\frac{ \\pi_c N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x_p})}{\\sum_{c'} \\pi_c N(\\mathbf{\\mu_{c'}},\\mathbf{\\Sigma_{c'}} )(\\mathbf{x_p})}, c=1...N_c$$\n",
        "\n",
        ", where $\\mathbf{x_p}$ are the coordinates of point p.\n",
        "\n",
        "This formula follows from our probabilistic interpretation and Bayes theorem:\n",
        "\n",
        "$$f_p(c)=p(\\hat{C}=c|\\hat{X}=\\mathbf{x_p}) \\sim p(\\hat{X}=\\mathbf{x_p}|\\hat{C}=c) p(\\hat{C}=c)=N(\\mathbf{\\mu_c},\\mathbf{\\Sigma_c} )(\\mathbf{x_p})\\pi_c $$\n",
        "\n",
        "- *AIC/BIC:* after each clustering two numbers are returned. These can be used to select the optimal number of Gaussians to be used, similar to the Silhouette score. ( AIC and BIC consider both the likelihood of the data given the parameters and the complexity of the model related to the number of Gaussians used ). The lowest AIC or BIC value is an indication of a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "fdddJhgl952H"
      },
      "source": [
        "#### Sklearn: implementation and usage of Gaussian mixtures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "PCyazHHp952W"
      },
      "source": [
        "First of all, we see how the Gaussian model behaves on our original example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "MTu1jVUr952Y",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "points=km_load_th1()\n",
        "\n",
        "aic=[]\n",
        "bic=[]\n",
        "sil=[]\n",
        "\n",
        "for i_comp in range(2,6):\n",
        "    plt.figure()\n",
        "    plt.title(str(i_comp))\n",
        "    clf = GaussianMixture(n_components=i_comp, covariance_type='full')\n",
        "    clf.fit(points)\n",
        "    cluster_labels=clf.predict(points)\n",
        "    plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "    print(i_comp,clf.aic(points),clf.bic(points))\n",
        "    score=silhouette_score(points,cluster_labels)\n",
        "    aic.append(clf.aic(points))\n",
        "    bic.append(clf.bic(points))\n",
        "    sil.append(score)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "iVTi69ZS952o"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(2,6),aic,'-o')\n",
        "plt.title('aic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),bic,'-o')\n",
        "plt.title('bic')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(2,6),sil,'-o')\n",
        "plt.title('silhouette')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "W_RrKQ0W952z"
      },
      "source": [
        "So in this case we get a comparable result, and also the probabilistic tools agree with the Silhouette score ! Let's see how the Gaussian mixtures behave in the examples where K-means was failing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "cKmBGlDr9521"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th1()\n",
        "clf = GaussianMixture(n_components=3, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.title('K-Means')\n",
        "plt.xlim(-6,6)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hidden": true,
        "id": "4ubCBvfj952-"
      },
      "outputs": [],
      "source": [
        "points=gm_load_th2()\n",
        "clf = GaussianMixture(n_components=2, covariance_type='full')\n",
        "clf.fit(points)\n",
        "cluster_labels=clf.predict(points)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels, s=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_p=clf.predict_proba(points)\n",
        "e = entropy(cluster_p, axis=1)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_p[:, 0], s=2)\n",
        "plt.show()\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_p[:, 1], s=2)\n",
        "plt.show()\n",
        "plt.figure(figsize=(5,5))\n",
        "cm = plt.cm.get_cmap('RdYlBu')\n",
        "sc = plt.scatter(points[:,0],points[:,1],c=e, s=2, cmap=cm)\n",
        "plt.colorbar(sc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3krnU_ndC_nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIykj5dp953I"
      },
      "source": [
        "#### EXERCISE 4: Find the prediction uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-y4HI-veff4"
      },
      "outputs": [],
      "source": [
        "#In this exercise you need to load the dataset used to present K-means ( def km_load_th1() ) or the one used to discuss\n",
        "# the Gaussian mixtures model ( def gm_load_th1() ).\n",
        "#As discussed, applying a fitting based on gaussian mixtures you can not only predict the cluster label for each point,\n",
        "#but also a probability distribution over the clusters.\n",
        "\n",
        "#From this probability distribution, you can compute for each point the entropy of the corresponging\n",
        "#distribution (using for example scipy.stats.entropy) as an estimation of the undertainty of the prediction.\n",
        "#Your task is to plot the data-cloud with a color proportional to the uncertainty of the cluster assignement.\n",
        "\n",
        "# In detail you shoud:\n",
        "# 1. Instantiate a GaussianMixture object with the number of clusters that you expect\n",
        "# 2. fit the object on the dataset with the fit method\n",
        "\n",
        "points=gm_load_th1()\n",
        "\n",
        "plt.figure()\n",
        "clf = GaussianMixture(..., covariance_type='full')\n",
        "clf...(...)\n",
        "\n",
        "# 3. compute the cluster probabilities using the method predict_proba. This will return a matrix of\n",
        "# dimension npoints x nclusters (check that this is the case!)\n",
        "# 4. use the entropy function ( from scipy.stats you need to import the entropy\n",
        "# function ) to evaluate for each point the uncertainty of the\n",
        "#prediction. Check here if in doubt: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
        "\n",
        "cluster_labels_prob=...\n",
        "\n",
        "from ... import ...\n",
        "entropies=[]\n",
        "for point in range(len(...)):\n",
        "    entropies.append(...)\n",
        "\n",
        "# 5. Plot the points colored accordingly to their uncertanty.\n",
        "\n",
        "cm = plt.cm.get_cmap('RdYlBu')\n",
        "sc = plt.scatter(points[:,0], points[:,1], c=entropies, cmap=cm)\n",
        "plt.colorbar(sc)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "9O3ArMG6eff4"
      },
      "source": [
        "### 4. Final comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "3EbOD7_Eeff5"
      },
      "source": [
        "We covered here the most basic clustering techniques, showcasing different behaviors. For real like projects, there are also other algorithms that could be taken into consideration, e.g.:\n",
        "\n",
        "- DBSCAN : https://en.wikipedia.org/wiki/DBSCAN\n",
        "- Spectral Clustering : https://towardsdatascience.com/spectral-clustering-aba2640c0d5b\n",
        "\n",
        "Both these methods can fit also clusters with weirder shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_65g5ZdTxCdm"
      },
      "source": [
        "<img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/clusters.png\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = DBSCAN(eps=3, min_samples=2)\n",
        "cluster_labels = clusterer.fit_predict(points)\n",
        "score=silhouette_score(points,cluster_labels)\n",
        "plt.scatter(points[:,0],points[:,1],c=cluster_labels)\n",
        "plt.show()\n",
        "print(f'n_cluster={max(cluster_labels)+1}, score={score}')"
      ],
      "metadata": {
        "id": "kflQ45bd8d3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dimensionality Reduction"
      ],
      "metadata": {
        "id": "YDsRQaj-r61V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afROKfT591kS"
      },
      "source": [
        "### 1. Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j318u5mjYBHI"
      },
      "source": [
        "Let's start with the example that we will use to make the theory more concrete. We will take a dataset from from kaggle https://www.kaggle.com/datasets/miroslavsabo/young-people-survey?resource=download (already downloaded for you in the folder `data`)\n",
        "\n",
        "The datasets consist of the results of a survey about the music preferences of several students, arriving at the following dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T14:09:42.322377Z",
          "start_time": "2022-09-27T14:09:42.283713Z"
        },
        "id": "D7yUYY_AYBHK"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"data/responses.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHBJAn1EYBHL"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5cOiQL6YBHL"
      },
      "outputs": [],
      "source": [
        "music_columns=data.columns[:19]\n",
        "print(music_columns)\n",
        "music_data=data[music_columns].dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjyjnYNDYBHN"
      },
      "source": [
        "The answers are of course correlated and we expect to have typical patterns recurring, that we define as people liking similar types of songs.\n",
        "\n",
        "The patterns may be also mixing, for e.g. a class of people may like classic `Pop` and `Reggae`, but not `Latino`. Another class may be `Latino` together with `Reggae`, but not `Pop`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAo57GQlYBHO"
      },
      "source": [
        "PCA will help to find these typical patterns and their number in a data driven fashion. As we will see these patterns will naturally appear when trying to compress data in a lower dimensional space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqNWQWT91kU"
      },
      "source": [
        "#### Theory overview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xgqBWg9YBHa"
      },
      "source": [
        "We will start looking at PCA from the point of view of `dimensionaliy reduction`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVw0KP4_91kW"
      },
      "source": [
        "**Objective:** PCA is used for dimensionality reduction when we have a large number $D$ of features with non-trivial intercorrelation ( data redundancy ) and to isolate relevant features. The number of features $D$ defines the original dimension of the dataset. Each sample defines a vector of dimensionality $D$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T12:13:53.622053Z",
          "start_time": "2022-09-27T12:13:53.611428Z"
        },
        "id": "DM8Z2LkpYBHc"
      },
      "source": [
        "PCA provides a new set of $M$ uncorrelated features for every data point, with $M \\le D$. The new features are:\n",
        "\n",
        "- a linear combination of the original ones ;\n",
        "- uncorrelated between each other ;\n",
        "\n",
        "If $M \\ll D$ we get an effective dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T12:17:23.351734Z",
          "start_time": "2022-09-27T12:17:23.345972Z"
        },
        "id": "f7baG0AYYBHc"
      },
      "source": [
        "    QUESTION: Does the number of data points changes after applying PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGrnICCAYBHf"
      },
      "source": [
        "PCA describes subspaces (lines,planes, and their high dimensional generalizations) as \"generated\" by a sequence of vectors $v_1,...,v_k$. This is defined as the set of all the vectors:\n",
        "\n",
        "$$V_{\\{v_1,...,v_k\\}}=\\{ \\alpha_1 v_1+...+\\alpha_k v_k, \\alpha_1,..., \\alpha_k \\in \\mathbf{R} \\}$$\n",
        "\n",
        "PCA will therefore provide a sequence of vectors. Since many choices of $\\{v_1,...,v_k\\}$ could describe the same \"line\", \"plane\", etc. etc., it imposes an orthonormality condition.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ySGaLWp91kZ"
      },
      "source": [
        "**Terminology and output of a PCA computation:**\n",
        "- `Principal components`: A sequence of orthonormal vectors $k_1,..,k_n$. We can interpret these vectors as the typical patterns found in the data, from increasing to decreasing probability of appearance. Any vector can be constructed as a linear combination of such patterns. Using only the first $m$ vectors we get a \"best approximation\".\n",
        "\n",
        "    \n",
        "- `Scores`: For every sample-point $p$, the new features are called scores are given by the component of $p$ along the $k$ vectors;  \n",
        "\n",
        "\n",
        "- `Reconstructed vector`: For every $k$, the projection of $V$ on $V_k$. This is the best approximation that we can get of the vector using only $k$ vectors (principal components).\n",
        "\n",
        "\n",
        "- `Explained variance`: For every k, the ratio between the variance of the reconstructed vectors and total variance. The number of components is chosen selecting an optimal k. The plot of the explained variance as a function of k is called a *scree plot*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF-FTX_P91ka"
      },
      "source": [
        "#### Sklearn: implementation and usage of PCA.\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGwCp73k91kb"
      },
      "source": [
        "We start showing a two-dimensional example that can be easy visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEWiYnL91ko"
      },
      "source": [
        "We load the datasets that we are going to use for the examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUjSpXxvYBHh"
      },
      "outputs": [],
      "source": [
        "data=load_sample_data_pca()\n",
        "\n",
        "n_samples,n_dim=data.shape\n",
        "\n",
        "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
        "\n",
        "plt.figure(figsize=((5,5)))\n",
        "plt.grid()\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvgzzfA91k8"
      },
      "source": [
        "The data set is almost one dimensional. PCA will confirm this result.\n",
        "\n",
        "As with most of sklearn functionalities, we need first to create a PCA object. We will use the object methods to perform PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-26T19:12:10.956955Z",
          "start_time": "2022-09-26T19:12:10.953963Z"
        },
        "id": "l6poli6o91k_"
      },
      "outputs": [],
      "source": [
        "pca=PCA(n_components=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju0UCpkP91lK"
      },
      "source": [
        "A call to the pca.fit method computes the principal components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-26T19:12:12.057604Z",
          "start_time": "2022-09-26T19:12:12.034521Z"
        },
        "id": "ifWK32ME91lM"
      },
      "outputs": [],
      "source": [
        "pca.fit(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZdfZbLv91lW"
      },
      "source": [
        "Now the pca.components_ attribute contains the principal components. We can print them alongside with the data and check that they constitute an orthonormal basis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxtGI26FYBHm"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=((5,5)))\n",
        "plt.grid()\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "\n",
        "circle=plt.Circle((0, 0), 1.0, linestyle='--', color='red',fill=False)\n",
        "ax=plt.gca()\n",
        "ax.add_artist(circle)\n",
        "\n",
        "for vec in pca.components_:\n",
        "    plt.quiver([0], [0], [vec[0]], [vec[1]], angles='xy', scale_units='xy', scale=1)\n",
        "\n",
        "plt.xlim(-2,2)\n",
        "plt.ylim(-2,2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q48o3r3t91li"
      },
      "source": [
        "The pca.explained_variance_ratio_ attribute contains the explained variance. In this case we see that already the first reconstructed vector explains 95% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-26T19:12:22.269942Z",
          "start_time": "2022-09-26T19:12:22.263717Z"
        },
        "id": "TwEopdTN91lk"
      },
      "outputs": [],
      "source": [
        "print(pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfaA1wo391ls"
      },
      "source": [
        "To compute the reconstructed vectors for k=1 we first need to compute the scores and then multiply by the basis vectors:\n",
        "\n",
        "$\\mathbf x_{rec}=\\sum_i (\\mathbf x \\cdot \\mathbf v^{pr}_i) \\mathbf v^{pr}_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-26T19:12:24.347892Z",
          "start_time": "2022-09-26T19:12:24.343929Z"
        },
        "id": "enRkHTyO91lu"
      },
      "outputs": [],
      "source": [
        "k=1\n",
        "scores=pca.transform(data)\n",
        "res=np.dot(scores[:,:k], pca.components_[:k,:] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnTRXSw2YBHp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=((5,5)))\n",
        "plt.plot(res[:,0],res[:,1],'o')\n",
        "plt.plot(data[:,0],data[:,1],'o')\n",
        "\n",
        "for a,b,c,d in zip(data[:,0],data[:,1],res[:,0],res[:,1]) :\n",
        "    plt.plot([a,c],[b,d],'-', linestyle = '--', color='red')\n",
        "\n",
        "plt.grid()\n",
        "\n",
        "plt.xlim(-1.0,1.0)\n",
        "plt.ylim(-1.0,1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlmDioD91mD"
      },
      "source": [
        "The same procedure is followed for high dimensional datasets. Here we generate random data which lies almost on a 6-dimensional subspace. The resulting scree plot can be used to find this result in a semi-automatic fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpw2Q8ZdYBHq"
      },
      "source": [
        "Let's redo the same now with our survey dataset, to review the concepts again and think \"high\"-dimensionally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T14:44:10.677074Z",
          "start_time": "2022-09-27T14:44:10.652371Z"
        },
        "id": "29Rp7VKPYBHr"
      },
      "outputs": [],
      "source": [
        "pca=PCA()\n",
        "pca.fit(music_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEEf_-GLYBHr"
      },
      "outputs": [],
      "source": [
        "plt.plot(pca.explained_variance_ratio_,'-o', label='explained variance')\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_),'-o', label='cumsum(explained variance)')\n",
        "plt.legend()\n",
        "plt.ylim(0, 1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjhrGPhOYBHs"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(pca.components_.transpose(),\n",
        "                  columns = [f'V_{i+1}' for i in range(len(music_columns))],\n",
        "                  index=music_columns)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u8W-oa-YBHs"
      },
      "outputs": [],
      "source": [
        "for vector in ['V_1','V_2','V_3','V_4']:\n",
        "    plt.figure()\n",
        "    plt.title(vector)\n",
        "    plt.plot(np.arange(len(music_columns)),list(df[vector]),'-o')\n",
        "    _=plt.xticks(np.arange(len(music_columns)),music_columns, rotation=90)\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T12:40:33.906396Z",
          "start_time": "2022-09-27T12:40:33.902364Z"
        },
        "id": "h3WLI4EuYBHt"
      },
      "source": [
        "#### EXERCISE 1 : Find the dimensionality of the hidden dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAxKgF_UFkDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od0o_4HoYBHt"
      },
      "outputs": [],
      "source": [
        "# In this exercise you will take a custom high dimensional dataset and try to find its dimensionality\n",
        "\n",
        "# 1. Load the data using the function data=load_multidimensional_data_pca2() , check the dimensionality of the data and plot them.\n",
        "data=...\n",
        "n_samples,n_dim=...\n",
        "print('We have ',n_samples, 'samples of dimension ', n_dim)\n",
        "\n",
        "# 2. Define a PCA object and perform the PCA fitting.\n",
        "pca=PCA()\n",
        "pca....\n",
        "\n",
        "# 3. Check the explained variance ratio and select best number of components.\n",
        "print(pca...)\n",
        "plt.plot(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nJsEk2R91nM"
      },
      "source": [
        "#### EXERCISE 2 : Find the dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnh72-8uYBHt"
      },
      "outputs": [],
      "source": [
        "k = #\n",
        "scores=pca.transform(data)\n",
        "\n",
        "# visualize the `scores.T[:k]` with a scatterplot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3AtpLmQef_R"
      },
      "source": [
        "### Final comments:\n",
        "\n",
        "PCA is able therefore to make this mapping:\n",
        "\n",
        "$(x_1,...,x_D) \\rightarrow (y_1,..,y_M)$\n",
        "\n",
        "Here we focused on data compression, but it is also very important that $y_1,...,y_M$ are uncorrolated for interpratibility purposes. Being uncorrelated means (roughly) that in our dataset we can change one variable without affecting the others. The dimensions 1,...,M are often therefore more interpretable and providing more information.\n",
        "\n",
        "See e.g. a similar application here:\n",
        "\n",
        "\"Principal component analysis of dietary and lifestyle patterns in relation to risk of subtypes of esophageal and gastric cancer\", https://pubmed.ncbi.nlm.nih.gov/21435900/\n",
        "\n",
        ", where each data point $x$ is an answer from a questionnaire of food. The principal components are than typical \"patterns\" of answers that are uncorrlated, have a look at table 2, and if you want read the whole data story :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEK5-Ksg91nw"
      },
      "source": [
        "### 2. Data visualization and embedding in low dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rP48tTL91nz"
      },
      "source": [
        "#### Theory overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Jer5L-91n0"
      },
      "source": [
        "PCA is a linear embedding technique where the scores are a linear function of the original variables. This forces the number of principal components to be used to be high, if the manifold is highly non-linear. Curved manifolds need to be embedded in higher dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX4zvhaN91n2"
      },
      "source": [
        "Other `non-linear` embedding techniques start from a local description of the environment of each sample point in the original space. `UMAP` describes the `topology` of the environment through a generalized \"triangulation\" (simplex decomposition).\n",
        "\n",
        "The projection on the low-dimensional space is optimized in order to match as much as possible the description of the local environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhP5EIc791n7"
      },
      "source": [
        "The embedding is given by an iterative solution of a minimization problem and therefore the results may depend on the value of the random seed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-iq56Yn91oD"
      },
      "source": [
        "#### Example 1: Exercise 3 Cont'd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZ_RAwj91oE"
      },
      "source": [
        "We will first visualize our multi-dimensional heart using t-SNE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-26T19:26:26.119196Z",
          "start_time": "2022-09-26T19:26:25.619284Z"
        },
        "id": "8TFExSbC91oF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data = load_multidimensional_data_pca2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0R8p_0E91oR"
      },
      "source": [
        "And using UMAP :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUU2YauU91oT"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=50, n_components=2, random_state=1711)\n",
        "\n",
        "emb = umap_model.fit_transform(data)\n",
        "plt.scatter(emb[:, 0], emb[:, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPOM-hCV91pB"
      },
      "source": [
        "#### Example 2: Fashion_Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq3DPRiV91pF"
      },
      "outputs": [],
      "source": [
        "fmnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fmnist.load_data()\n",
        "\n",
        "n_examples = 5000\n",
        "data=train_images[:n_examples,:].reshape(n_examples,-1)\n",
        "data=data/255\n",
        "\n",
        "labels=train_labels[:n_examples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-09-27T15:07:26.743568Z",
          "start_time": "2022-09-27T15:07:26.738108Z"
        },
        "id": "UUnsyxnet9n9"
      },
      "source": [
        " | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/t_sne_fmnist.png\" width=\"100%\"/> | <img src=\"https://github.com/neworldemancer/DSF5/raw/master/figures/fmnist.png\" width=\"100%\"/>\n",
        " |  -----:| -----:|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq3vZFJy91p8"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=50, n_components=2, random_state=1711)\n",
        "umap_fmnist = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_fmnist[:, 0], umap_fmnist[:, 1], c=labels, s=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz2LEGDP91qM"
      },
      "source": [
        "#### Example 3: House prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eycvatmq91qO"
      },
      "outputs": [],
      "source": [
        "data, price =house_prices_dataset()\n",
        "data = StandardScaler().fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWeCP6pN91qr"
      },
      "outputs": [],
      "source": [
        "umap_model = umap.UMAP(n_neighbors=30, n_components=2, random_state=1711)\n",
        "umap_houses = umap_model.fit_transform(data)\n",
        "plt.scatter(umap_houses[:, 0], umap_houses[:, 1], s=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3wGs2Q91rE"
      },
      "source": [
        "**Message:** Visualization techniques are useful for having an initial grasp of multi-dimensional datasets and guide further analysis and the choice of the modelling data strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78xdYF9-B-jA"
      },
      "source": [
        "**Caveats:**\n",
        "- UMAP, thanks to the algorithm being amanable to clever initializations and optimization schemes, offers great stability and scaling properties\n",
        "\n",
        "- UMAP, even if starting from a local picture, is able to spread apart different clusters\n",
        "\n",
        "- The result of an embedding may depend on the values of the metaparameters. One should try to see how the final embedding changes in order to get to a complete picture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Path to success"
      ],
      "metadata": {
        "id": "GmI6BdVg8RB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Learn the tools: shortcuts, visualisation, ...\n",
        "\n",
        "0. Explore the data\n",
        "1. Data preprocessing: reading data, dealing with outliers/NANs/categorical vars, feature selection\n",
        "\n",
        "\n",
        "2. From simple models to complex.\n",
        "3. Independent train/val/test datasets.\n",
        "7. Optimization: n_jobs=-1\n",
        "4. Appropriate metrics: `metrics.recall_score()`, `metrics.precision_score()`, `metrics.f1_score()`\n",
        "5. Interpreting the result\n",
        "6. Search for best hyperparameters\n"
      ],
      "metadata": {
        "id": "2Z-4WA7K8qcL"
      }
    }
  ]
}